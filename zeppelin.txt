1. Load Data
var fileName = "/home/vagrant/data/age.csv"
var file = sc.textFile(fileName)

case class WellBeing(minAge: Integer, maxAge: Integer, male:Float, female: Float, total: Float)

// for (line <- lines) println(line)

val wb = file
            .map(l => l.split(","))
            .filter(_(0) != "Age")
            .map(l => WellBeing(
                l(0).split("-")(0).toInt,
                l(0).split("-")(1).toInt,
                l(1).toFloat,
                l(2).toFloat,
                (l(1).toFloat + l(2).toFloat) / 2))

// wb.collect().foreach(println)

2. Create Data Frame
// Zeppelin creates and injects sc (SparkContext) and sqlContext (HiveContext or SqlContext)
// So you don't need create them manually (val sqlContext = new org.apache.spark.sql.SQLContext(sc))
//import sqlContext.implicits._

val df = wb.toDF()

df.show()
df.printSchema()
df.select("minAge").show()

df.registerTempTable("dataframe1")

sqlContext.tableNames().foreach(println)

val retiredPeople = sqlContext.sql("SELECT * FROM df WHERE minAge > 65")
retiredPeople.collect().foreach(println)

3. Show Simple Statistics
import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}
import org.apache.spark.mllib.linalg.{Vector, Vectors}

println(wb.count) + " records found."

// create vectors
var vectors = wb.map(x => Vectors.dense(x.minAge.toFloat, x.maxAge.toFloat, x.male, x.female, x.total))

val summary = Statistics.colStats(vectors)

println("mean values: " + summary.mean)
println("variances: " + summary.variance)

4. Correlations
var corr = Statistics.corr(vectors, "pearson")

println
println("Correlations:")
println(corr.toString(10, 1000))

var vectors_old = wb.filter(_.minAge >= 50).map(x => Vectors.dense(x.minAge.toFloat, x.maxAge.toFloat, x.male, x.female, x.total))
var corr_old = Statistics.corr(vectors_old, "pearson")

println
println("Correlations over 50:")
println(corr_old.toString(10, 1000))

var vectors_young = wb.filter(_.minAge < 50).map(x => Vectors.dense(x.minAge.toFloat, x.maxAge.toFloat, x.male, x.female, x.total))
var corr_young = Statistics.corr(vectors_young, "pearson")

println
println("Correlations under 50:")
println(corr_young.toString(10, 1000))

5. Linear Regression
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.regression.LinearRegressionModel
import org.apache.spark.mllib.regression.LinearRegressionWithSGD
import org.apache.spark.mllib.classification.LogisticRegressionWithSGD
import org.apache.spark.mllib.linalg.Vectors

// nonlinear regression is not part of Spark yet...
// split data into two parts: before and after 50. (u-shape of data)
// linear regression with stochastic gradient descent needs scaled data; divide the happiness scores by 10 to put them in a 0..1 range

// LabeledPoint: class that represents the features and labels of a data point. LabeledPoint(double label, Vector features)
val data_young = wb.filter(_.minAge < 50).map(line => LabeledPoint(line.total/10, Vectors.dense(line.minAge.toDouble/100, line.maxAge.toDouble/100))).collect()
val rdd_young = sc.makeRDD(data_young)

val data_old = wb.filter(_.minAge >= 50).map(line => LabeledPoint(line.total/10, Vectors.dense(line.minAge.toDouble/100, line.maxAge.toDouble/100))).collect()
val rdd_old = sc.makeRDD(data_old)

val numIterations = 100
// step size is learning rate.
val stepSize = 0.5

// train < 50
val algorithm_young = new LinearRegressionWithSGD()
algorithm_young.setIntercept(true).setValidateData(false)
algorithm_young.optimizer.setNumIterations(numIterations).setStepSize(stepSize)
val model_young = algorithm_young.run(rdd_young)

// train > 50
val algorithm_old = new LinearRegressionWithSGD()
algorithm_old.setIntercept(true).setValidateData(false)
algorithm_old.optimizer.setNumIterations(numIterations).setStepSize(stepSize)
val model_old = algorithm_old.run(rdd_old)

//model.save(sc, "/home/vagrant/models/linearRegressionUnder50")

println(model_young.intercept)
println(model_young.weights)
println(model_old.intercept)
println(model_old.weights)

6. Evaluate model
// Evaluate young and old models on training examples and compute training error
val valuesAndPreds_young = data_young.map { point =>
  val prediction = model_young.predict(point.features)
  println("Prediction YOUNG: point=" + point.label + ", features=" + point.features + ", prediction=" + prediction)
  (point.label, prediction)
}
val MSE_young = valuesAndPreds_young.map{case(v, p) => math.pow((v - p), 2)}

val valuesAndPreds_old = data_old.map { point =>
  val prediction = model_old.predict(point.features)
  println("Prediction OLD: point=" + point.label + ", features=" + point.features + ", prediction=" + prediction)
  (point.label, prediction)
}
val MSE_old = valuesAndPreds_old.map{case(v, p) => math.pow((v - p), 2)}

// MSE incidates the goodness of fit; should be smaller than 1
println("Young training Mean Squared Error = " + (MSE_young.sum / MSE_young.size))
println("Old training Mean Squared Error =   " + (MSE_old.sum / MSE_old.size))

7. Predict
import org.apache.spark.mllib.linalg._

val v:Vector = new DenseVector(Array(30))
model.predict(v)


===================================
1. Load data
var fileName = "/home/vagrant/data/nhisfamily.csv"
var file = sc.textFile(fileName)

case class NHIS(FM_SIZE: Double, FM_KIDS: Double, FM_EDUC1: Double, // family education
                FHSTATVG: Double, FHSTATEX: Double, health: Double,  // health: number of familiy members whose health is very good or excellent, divided by number of family members.
                RAT_CAT2: Double, wealth: Double)  // wealth = ratio of family income to the poverty treshold. skip: 15,16,17,96,99

val records = file.map(l => l.split(","))  // comma-separated file
            .filter(_(0) != "RECTYPE")   // skip header
            .filter(_(121).toInt < 16)   // skip records with undefinable, unknown or unclear poverty ratios
            .map(l => NHIS(l(16).toDouble, l(17).toDouble, l(22).toDouble,
                           l(40).toDouble, l(39).toDouble, (l(40).toDouble + l(39).toDouble) / l(16).toInt,
                           l(121).toDouble, l(121).toDouble))


records.count()
records.take(10).foreach(println)

2. Data Checks
import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}
import org.apache.spark.mllib.linalg.{Vector, Vectors}

println(file.count() + " lines in file")
println(records.count() + " records found")
records.take(10).foreach(println)

val df = records.toDF()
df.registerTempTable("dataframe1")

sqlContext.sql("SELECT * FROM dataframe1 WHERE Health > 1").count()    // should be 0

var vectors = records.map(x => Vectors.dense(x.FamilySize, x.Kids, x.Education, x.Health, x.Wealth))

val summary = Statistics.colStats(vectors)
println("mean values: " + summary.mean)
println("variances: " + summary.variance)

3. Correlations
var corr = Statistics.corr(vectors, "pearson")

println
println(corr.toString(10, 1000))

4. Linear Regression
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.regression.LinearRegressionModel
import org.apache.spark.mllib.regression.LinearRegressionWithSGD

// Health
println("HEALTH predictions")
val data_health = records.map(record => LabeledPoint(record.Health, Vectors.dense(record.FamilySize/10, record.Kids/10, record.Education/10))).collect()
val rdd_health = sc.makeRDD(data_health)

val splits_health = rdd_health.randomSplit(Array(0.8, 0.2))
val training_health = splits_health(0).cache()
val test_health = splits_health(1).cache()

val algorithm_health = new LinearRegressionWithSGD()
algorithm_health.setIntercept(true)
// algorithm_health.optimizer.setNumIterations(100).setStepSize(0.1)
val model_health = algorithm.run(training_health)

println(model_health.intercept)
println(model_health.weights)

val prediction_health = model_health.predict(test_health.map(_.features))

val predictionAndLabel_health = prediction_health.zip(test_health.map(_.label))
predictionAndLabel_health.count()
predictionAndLabel_health.take(20).foreach((p) => println("HEALTH prediction=" + p._1 + ", actual=" + p._2))

//model_health.save(sc, "/home/vagrant/models/health")
println()

// Wealth
println("WEALTH predictions")
val data_wealth = records.map(record => LabeledPoint(record.Wealth/10, Vectors.dense(record.FamilySize/10, record.Kids/10, record.Education/10))).collect()
val rdd_wealth = sc.makeRDD(data_wealth)

val splits_wealth = rdd_wealth.randomSplit(Array(0.8, 0.2))
val training_wealth = splits_wealth(0).cache()
val test_wealth = splits_wealth(1).cache()

val algorithm_wealth = new LinearRegressionWithSGD()
algorithm_wealth.setIntercept(true)
// algorithm_wealth.optimizer.setNumIterations(100).setStepSize(0.1)
val model_wealth = algorithm.run(training_wealth)

println(model_wealth.intercept)
println(model_wealth.weights)

val prediction_wealth = model_wealth.predict(test_wealth.map(_.features))

val predictionAndLabel_wealth = prediction_wealth.zip(test_wealth.map(_.label))
predictionAndLabel_wealth.count()
predictionAndLabel_wealth.take(20).foreach((p) => println("WEALTH prediction=" + p._1 + ", actual=" + p._2))

//model_wealth.save(sc, "/home/vagrant/models/wealth")

5. Predict
import org.apache.spark.mllib.linalg._

val v1:Vector = new DenseVector(Array(0.4, 0.2, 0.9))  // 4 people, 2 kids, master's degree
model_health.predict(v1)
model_wealth.predict(v1)

val v2:Vector = new DenseVector(Array(0.4, 0.2, 0.2))  // 4 people, 2 kids, no high school diploma
model_health.predict(v2)
model_wealth.predict(v2)


